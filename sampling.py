## The number of sampling steps.
import tqdm
import torch
import numpy as np


def langevin(
    model,
    test_set=None,
    device="cpu",
    batch_size=500,
    num_steps=500,
    snr=0.16,
    eps=1e-3,
    init_state="testset",
):
    """
    Parameters
    ----------
    model : torch.nn.Module
        Model whose forward method returns negative log-likelihood of observations
    test_set : torch.utils.data.DataSet
        a dataset object which is used when init_state is "testset"
    device : str
        device to run computations on. 
    batch_size : int
        number of samples to generate
    num_steps : int
        how many iterations of Langeving sampling to be run before getting final samples
    init_state : str
        One of rand, randn, or testset, setting the initial state of the sampling.

    Returns
    -------
    x : torch.Tensor
        Tensor containing samples generated by the sampling
    init_x : torch.Tensor
        Corrsponding intial states of samples generated
    """
    if init_state == "rand":
        init_x = torch.rand(batch_size, 28 * 28).to(device)
    elif init_state == "randn":
        init_x = torch.randn(batch_size, 28 * 28).to(device)
    elif init_state == "testset":
        sample_loader = torch.utils.data.DataLoader(
            test_set, batch_size=batch_size, shuffle=True, num_workers=0
        )
        init_x, _ = next(iter(sample_loader))
        init_x = init_x.reshape([batch_size, 28 * 28]).to(device)
    else:
        init_x = init_state.squeeze(1).to(device)
        init_x = init_x.flatten(start_dim=1)
        if tuple(init_x.shape) != (batch_size, 28 * 28):
            raise ValueError

    time_steps = np.linspace(1.0, eps, num_steps)
    x = init_x
    for time_step in tqdm.notebook.tqdm(time_steps):
        dup_x = torch.clone(x).detach()
        dup_x.requires_grad_(True)
        logpt_x = -model(dup_x).sum()
        grad = torch.autograd.grad(logpt_x, dup_x, create_graph=True)[0]
        with torch.no_grad():
            grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()
            noise_norm = np.sqrt(np.prod(x.shape[1:]))
            langevin_step_size = 2 * (snr * noise_norm / grad_norm) ** 2
            x = (
                x
                + langevin_step_size * grad
                + torch.sqrt(2 * langevin_step_size) * torch.randn_like(x)
            )
    return x, init_x
